Large Language Models (LLMs) have taken the world by storm especially after OpenAI released
ChatGPT and raised public awareness of the power that the ‘transformer’ neural network architecture has when it comes to learning underlying patterns in human language (GPT stands for
Generative Pre-trained Transformer). In this investigation you will be working with a tiny subset
of language, specifically, a list of names. The transformer architecture uses ‘deep neural networks’
and is outside the scope of this investigation. Instead, you will be guided to design a computational
algorithm that can ‘learn’ the statistics of the characters that make up the provided names, and
then use the mathematics of probability to generate new names from the learned statistics. Keep
in mind that the transformer architecture used in LLMs is a sophisticated means of learning the
statistics of human language from large databases of online text and storing those statistics in a
neural network, and then prompting the network to probabilistically generate human-like language.
As you go through the algorithm design process, make sure to document your progress and all
assumptions made in presentation slides. The designs in your slides should be in plain English
using brief dot-points (with indentation to indicate blocks of instructions) or you can use diagrams,
pseudocode or flowcharts if you prefer. It is important that you demonstrate a deep understand all
formulae that you use – document these in your slides with an explanation of how they work.
